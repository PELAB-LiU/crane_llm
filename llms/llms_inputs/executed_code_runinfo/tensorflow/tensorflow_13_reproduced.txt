# Executed Cells:
## Cell 1:

import numpy as np


def load_glove_embeddings(embeddings_file):
    embeddings_index = dict()
    with open(embeddings_file, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            coefs = np.asarray(values[1:], dtype='float32')
            embeddings_index[word] = coefs
    return embeddings_index

glove_embeddings_file = 'data/glove.6B.50d.txt'
glove_embeddings = load_glove_embeddings(glove_embeddings_file)

## Cell 2:

from transformers import BertTokenizer


bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')


def tokenize_text(text):
    tokens = bert_tokenizer.tokenize(text)
    tokens = ['[CLS]'] + tokens + ['[SEP]']
    input_ids = bert_tokenizer.convert_tokens_to_ids(tokens)
    return input_ids


text = "This is an example sentence."
input_ids = tokenize_text(text)

## Cell 3:

docs = np.array(['Well done!',
  'Good work',
  'Great effort',
  'nice work',
  'Excellent!',
  'Weak',
  'Poor effort!',
  'not good',
  'poor work',
  'Could have done better.'])

labels = np.array([1,1,1,1,1,0,0,0,0,0])

# Current relevent runtime information:
{'__method__np_array': {'type': "<class 'builtin_function_or_method'>"},
 'docs': {'dtype': '<U23',
          'execution_cell_source': {'cellno': 9, 'lineno': 1},
          'shape': (10,),
          'type': 'numpy.ndarray',
          'value_info': {'num_unique': 10,
                         'value_type': 'categorical or object'}},
 'np': {'type': "<class 'module'>"},
 'tokenize_text': {'execution_cell_source': {'cellno': 1, 'lineno': 7},
                   'type': "<class 'function'>"}}
# Target Cell:

import numpy as np
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

max_length = 768

docs = np.array([np.array(pad_sequences(tokenize_text(i)), maxlen=max_length) for i in docs])