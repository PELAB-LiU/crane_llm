# Executed Cells:
## Cell 1:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
%matplotlib inline


np.random.seed(2)

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

import itertools

from keras.utils import to_categorical


from keras.models import Sequential

from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D

from keras.optimizers import RMSprop


from tensorflow.keras.preprocessing.image import ImageDataGenerator


from keras.callbacks import ReduceLROnPlateau



sns.set(style='white', context='notebook', palette='deep')

## Cell 2:

import tensorflow as tf


## Cell 3:

train = pd.read_csv("data/train.csv")
test = pd.read_csv("data/test.csv")

## Cell 4:

Y_train = train["label"]


X_train = train.drop(labels = ["label"],axis = 1)


del train

g = sns.countplot(Y_train)

Y_train.value_counts()

## Cell 5:


X_train = X_train / 255.0
test = test / 255.0

## Cell 6:


X_train = X_train.values.reshape(-1,28,28,1)
test = test.values.reshape(-1,28,28,1)

## Cell 7:


Y_train = to_categorical(Y_train, num_classes = 10)

## Cell 8:

random_seed = 2
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)

## Cell 9:

'''
- filters：滤波器的数量，即输出的通道数；
- kernelsize：卷积核的大小，这里是一个5x5的矩阵；
- padding：卷积的方式，这里使用'Same'表示输出图像的大小与输入图像相同；
- activation：激活函数，这里使用'Relu'函数；
- inputshape：输入数据的形状，这里是一个28x28的灰度图像（深度为1）。
'''

import keras
model = keras.models.Sequential([
    Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(28, 28, 1)),
])
print(X_train[0].shape)
p = model.predict(X_train[0:3])
p.shape

## Cell 10:




model = Sequential()



model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',
                 activation ='relu', input_shape = (28,28,1)))

model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',
                 activation ='relu'))

model.add(MaxPool2D(pool_size=(2,2)))


model.add(Dropout(0.25))

model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',
                 activation ='relu'))


model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',
                 activation ='relu'))


model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))


model.add(Dropout(0.25))


model.add(Flatten())


model.add(Dense(256, activation = "relu"))
model.add(Dropout(0.5))
model.add(Dense(10, activation = "softmax"))

## Cell 11:


optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)
'''
“decay=0”是RMSprop优化算法的一个参数，它控制了学习率的衰减。
具体来说，在RMSprop算法中，每个权重参数都有自己的学习率
，而decay参数会使这个学习率随着时间的推移而逐渐变小。
但是，由于decay=0，因此该算法不会在训练过程中降低学习率。

“epsilon=1e-08”是RMSprop优化算法的一个参数，它是用来防止除零错误的一个小量，通常取极小的值（例如1e-8）。
在RMSprop算法中，计算梯度平方平均值时需要对平方梯度加上一个极小的值，以避免出现除以零的错误。

“rho=0.9”是一种优化算法的参数，用于控制梯度的平滑度。具体来说，该参数决定了在计算平方梯度的指数移动平均值时，
历史数据的重要性。较高的rho值可以使平均值对历史数据的依赖性更强，从而使整个优化过程更加稳定。
'''

## Cell 12:


model.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])

## Cell 13:




learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
                                            patience=3,
                                            verbose=1,
                                            factor=0.5,
                                            min_lr=0.00001)
'''

在您提供的代码中，learningratereduction是ReduceLROnPlateau回调函数的一个实例，用于在模型训练过程中动态地减小学习率。
具体来说，它监视了验证集的准确性（即'monitor'='valacc'），并且如果在3个时期内没有改进，
则减小学习率（即'patience'=3）。该调用还指定了减小因子（即'factor'=0.5）和最小学习率（即'minlr'=0.00001），
以便在执行减少操作时进行限制，从而保持学习率的稳定性和有效性。如果您想要更好地了解ReduceLROnPlateau的工作原理和参数设置，

verbose是ReduceLROnPlateau回调函数的一个可选参数，用于控制输出详细程度的标志。
如果verbose=1，则在执行时期减少操作时将输出一条消息，以指示学习率的更新和当前的状态。
如果verbose=0，则不会输出任何消息。通常情况下，verbose的默认值为0，因为它可以大大减少输出的噪声和干扰。
如果您需要更详细的输出和信息，可以将verbose的值设置为1或更高
可以查看Keras文档：https://keras.io/callbacks/#reducelronplateau。
'''

## Cell 14:


checkpoint_filepath = 'best_model.keras'
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_acc',
    mode='max',
    save_best_only=True)

## Cell 15:

epochs = 1
batch_size = 86

## Cell 16:

datagen = ImageDataGenerator(
        featurewise_center=False,
        samplewise_center=False,
        featurewise_std_normalization=False,
        samplewise_std_normalization=False,





        zca_whitening=False,
        rotation_range=10,
        zoom_range = 0.1,
        width_shift_range=0.1,
        height_shift_range=0.1,

        horizontal_flip=False,
        vertical_flip=False)




datagen.fit(X_train)

## Cell 17:






history = model.fit(datagen.flow(X_train,Y_train, batch_size=batch_size),
                              epochs = epochs, validation_data = (X_val,Y_val),
                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size
                              , callbacks=[learning_rate_reduction,model_checkpoint_callback])

# Current relevent runtime information:
{'__method__history_history': {'detected': 'training_history_dict',
                               'length': 5,
                               'metrics': ['accuracy',
                                           'loss',
                                           'val_accuracy',
                                           'val_loss',
                                           'learning_rate'],
                               'n_epochs': 1,
                               'type': "<class 'dict'>"},
 '__method__plt_subplots': {'type': "<class 'function'>"},
 'history': {'execution_cell_source': {'cellno': 27, 'lineno': 6},
             'type': "<class 'keras.src.callbacks.history.History'>"},
 'plt': {'type': "<class 'module'>"}}
# Target Cell:

fig, ax = plt.subplots(2,1)
ax[0].plot(history.history['loss'], color='b', label="Training loss")
ax[0].plot(history.history['val_loss'], color='r', label="validation loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)




ax[1].plot(history.history['accuracy'], color='b', label="Training accuracy")
ax[1].plot(history.history['val_accuracy'], color='r',label="Validation accuracy")

legend = ax[1].legend(loc='best', shadow=True)