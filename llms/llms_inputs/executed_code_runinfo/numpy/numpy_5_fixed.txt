# Executed Cells:
## Cell 1:

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import time
from skimage.io import imshow
from IPython.display import display
from skimage.transform import resize
from tensorflow.keras.layers import Dense, BatchNormalization, LeakyReLU, Reshape
from tensorflow.keras.layers import Conv2DTranspose, Input, Concatenate, Conv2D
from tensorflow.keras.layers import Conv2D, Dropout, Flatten
from tensorflow.keras.models import Model
import os
import glob
from tensorflow.keras.callbacks import LambdaCallback


## Cell 2:

def Load_Pprepr_Data():
    Data = tf.keras.datasets.cifar10.load_data()
    (train_images , train_labels) , (test_images , test_labels) = Data
    return train_images ,train_labels ,test_images ,test_labels



def create_train_dataset(train_images, train_labels, batch_size):
    """
    Creates a TensorFlow dataset for training the AC-GAN model.

    Args:
        train_images (ndarray): Array of training images.
        train_labels (ndarray): Array of training labels.
        batch_size (int): Number of samples per batch.

    Returns:
        tf.data.Dataset: Training dataset.
    """


    BUFFER_SIZE = train_images.shape[0]


    train_dataset_y = tf.data.Dataset.from_tensor_slices(train_labels[:, 0]).map(lambda y: tf.one_hot(y, 10))


    train_dataset_x = tf.data.Dataset.from_tensor_slices(train_images)


    train_dataset = tf.data.Dataset.zip((train_dataset_x, train_dataset_y)).shuffle(BUFFER_SIZE).batch(batch_size)

    return train_dataset



def scale_image_to_float(image):
    """
    Scales the pixel values of an image to the range [-1, 1].

    Args:
        image (numpy.ndarray): Input image.

    Returns:
        numpy.ndarray: Image with pixel values scaled to the range [-1, 1].
    """
    return (image.astype(np.float32) - 127.5) / 127.5


def scale_image_to_uint8(image):
    """
    Scales the pixel values of an image to the range [0, 255] and converts them to uint8.

    Args:
        image (numpy.ndarray): Input image.

    Returns:
        numpy.ndarray: Image with pixel values scaled to the range [0, 255] and converted to uint8.
    """
    return np.clip((image * 127.5) + 128, 0, 255).astype(np.uint8)

## Cell 3:

class AC_GAN:



    def __init__(self):
        self.generator = None
        self.discriminator = None
        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
        self.discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)
        self.generator_gradients = None
        self.discriminator_gradients = None

        self.checkpoint_dir = './checkpoints'
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, 'ckpt')






    def build_generator(self,latent_dim, num_classes):
        """
        Builds the AC-GAN generator model.

        Args:
            latent_dim (int): Dimension of the noise input.
            num_classes (int): Number of classes/labels.

        Returns:
            generator (Model): The AC-GAN generator model.
        """

        noise_input = Input((latent_dim,))
        label_input = Input((num_classes,))


        x = Concatenate()([noise_input, label_input])


        x = Dense(4 * 4 * 256, use_bias=False)(x)
        x = BatchNormalization()(x)
        x = LeakyReLU()(x)


        x = Reshape((4, 4, 256))(x)


        x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', use_bias=False)(x)
        x = BatchNormalization()(x)
        x = LeakyReLU()(x)

        x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False)(x)
        x = BatchNormalization()(x)
        x = LeakyReLU()(x)

        x = Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same', use_bias=False)(x)
        x = BatchNormalization()(x)
        x = LeakyReLU()(x)


        output = Conv2D(3, (3, 3), strides=(1, 1), padding='same', activation='tanh')(x)


        generator = Model(inputs=[noise_input, label_input], outputs=output)

        return generator





    def build_discriminator(self,input_shape):
        """
        Build a discriminator model with the given input shape.

        Parameters:
            input_shape (tuple): Shape of the input images (height, width, channels).

        Returns:
            discriminator (Model): Discriminator model.

        """
        _i = Input(input_shape)
        _ = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(_i)
        _ = LeakyReLU()(_)
        _ = Conv2D(128, (3, 3), strides=(2, 2), padding='same', use_bias=False)(_)
        _ = BatchNormalization()(_)
        _ = LeakyReLU()(_)
        _ = Conv2D(128, (3, 3), strides=(2, 2), padding='same', use_bias=False)(_)
        _ = BatchNormalization()(_)
        _ = LeakyReLU()(_)
        _ = Conv2D(128, (3, 3), strides=(2, 2), padding='same', use_bias=False)(_)
        _ = BatchNormalization()(_)
        _ = LeakyReLU()(_)
        _ = Flatten()(_)
        _0 = Dense(1)(_)
        _1 = Dense(10)(_)

        discriminator = Model(inputs=_i, outputs=[_0, _1])
        return discriminator

    def prepare_generation(self,num_generated_examples, latent_dim):
        """
        Prepare vectors for image generation.

        Args:
            num_epochs (int): Number of training epochs.
            num_generated_examples (int): Number of images to generate.
            latent_dim (int): Dimension of the latent space vector.

        Returns:
            random_vector_for_generation (tf.Tensor): Random vector for generating images.
            condition_vector_generation (tf.Tensor): Condition vector for generating images.
        """

        random_vector_for_generation = tf.random.normal([num_generated_examples, latent_dim])
        condition_vector_generation = tf.one_hot(list(range(10)) * 2, 10)


        return random_vector_for_generation, condition_vector_generation



    def compute_compute_generator_loss(self,generated_output, labels):
        """
        Computes the generator loss for adversarial and class label matching.

        Args:
            generated_output (tuple): Tuple containing the discriminator output and class predictions.
            labels (tf.Tensor): True class labels.

        Returns:
            tf.Tensor: Total generator loss.

        """
        out_d, out_c = generated_output


        loss_discrit = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            labels=tf.ones_like(out_d), logits=out_d))


        loss_create = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
            labels=labels, logits=out_c))


        whole_loss = loss_discrit + loss_create
        return whole_loss





    def compute_compute_discriminator_loss(self,real_discriminator_output, generated_discriminator_output, true_labels):
        """
        Computes the discriminator loss for real and generated examples.

        Args:
            real_discriminator_output (tuple): Tuple containing the discriminator output and class predictions for real examples.
            generated_discriminator_output (tuple): Tuple containing the discriminator output and class predictions for generated examples.
            true_labels (tf.Tensor): True class labels.

        Returns:
            tf.Tensor: Total discriminator loss.

        """
        real_output_d, real_output_c = real_discriminator_output


        real_loss_d = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            labels=tf.ones_like(real_output_d), logits=real_output_d))
        real_loss_c = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
            labels=true_labels, logits=real_output_c))
        modified_real_loss = real_loss_d + real_loss_c

        generated_output_d, generated_output_c = generated_discriminator_output


        modified_generated_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            labels=tf.zeros_like(generated_output_d), logits=generated_output_d))


        modified_total_loss = modified_real_loss + modified_generated_loss

        return modified_total_loss




    def train_model(self,input_images, input_labels,latent_dim,BATCH_SIZE):
        """
        Performs a single training step for the generator and discriminator models.

        Args:
            input_images (tf.Tensor): Batch of real images.
            input_labels (tf.Tensor): Batch of labels for the images.

        Returns:
            None
        """



        noise = tf.random.normal([BATCH_SIZE, latent_dim])






        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:

            generated_images = self.generator([noise, input_labels], training=True)


            real_outputs = self.discriminator(input_images, training=True)
            generated_outputs = self.discriminator(generated_images, training=True)


            generator_loss = self.compute_compute_generator_loss(generated_outputs, input_labels)
            discriminator_loss = self.compute_compute_discriminator_loss(real_outputs, generated_outputs, input_labels)


        self.generator_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)
        self.discriminator_gradients = disc_tape.gradient(discriminator_loss,self.discriminator.trainable_variables)


        self.generator_optimizer.apply_gradients(zip(self.generator_gradients, self.generator.trainable_variables))
        self.discriminator_optimizer.apply_gradients(zip(self.discriminator_gradients, self.discriminator.trainable_variables))



    def plot_images(self,images,epoch):


        np.save("all_images.npy", np.array(images))
        print(f' generated images of epoch {epoch} , saved to all_images.npy')

        fig, axes = plt.subplots(nrows=2, ncols=10, figsize=(40, 20))


        axes = axes.flatten()


        for i in range(20):
            axes[i].imshow(images[i])
            axes[i].axis('off')



        fig.suptitle(f'Generated images of epoch :{epoch}', fontsize=42)


        plt.tight_layout()


        plt.show()

    def save_weights(self, checkpoint_name):

        old_weights = glob.glob(f"{self.checkpoint_prefix}*")
        for weight_file in old_weights:
            os.remove(weight_file)


        self.generator.save_weights(f"{self.checkpoint_prefix}_generator_{checkpoint_name}.weights.h5")
        self.discriminator.save_weights(f"{self.checkpoint_prefix}_discriminator_{checkpoint_name}.weights.h5")








    def train(self, train_dataset, latent_dim, num_class, num_examples_to_generate, epochs, batch_size=25):
        self.generator = self.build_generator(latent_dim, num_class)
        self.discriminator = self.build_discriminator((32, 32, 3))

        random_vector_for_generation, condition_vector_generation = self.prepare_generation(num_examples_to_generate,
                                                                                           latent_dim)
        generated = []

        for epoch in range(epochs):

            start_time = time.time()


            for images, labels in train_dataset:

                self.train_model(images, labels, latent_dim, batch_size)





            noise = tf.random.normal([num_examples_to_generate, latent_dim])


            generated_image = self.generator([noise, condition_vector_generation], training=False)
            print(np.array(generated_image).shape)

            self.plot_images(generated_image, epoch)
            generated.append(generated_image)


            print(epoch, time.time() - start_time)

            self.save_weights(f"epoch_{epoch}")

        return np.array(generated)











## Cell 4:

train_images ,train_labels ,test_images ,test_labels = Load_Pprepr_Data()
train_img_f32 = scale_image_to_float(train_images)
train_dataset = create_train_dataset(train_img_f32, train_labels, 25)

## Cell 5:

train_dataset_small = train_dataset.take(16)


print('start')
acgn = AC_GAN()


checkpoint_dir = 'checkpoints'


generator_weights_file = os.path.join(checkpoint_dir, 'ckpt_generator_epoch_2')
discriminator_weights_file = os.path.join(checkpoint_dir, 'ckpt_discriminator_epoch_2')


generator_files_exist = os.path.exists(generator_weights_file + '.data-00000-of-00001') and                        os.path.exists(generator_weights_file + '.index')
discriminator_files_exist = os.path.exists(discriminator_weights_file + '.data-00000-of-00001') and                            os.path.exists(discriminator_weights_file + '.index')




print(generator_files_exist)
print(discriminator_files_exist)
if(generator_files_exist and discriminator_files_exist):
    acgn.generator = acgn.build_generator(100, 10)
    acgn.discriminator = acgn.build_discriminator((32, 32, 3))
    acgn.generator.load_weights(generator_weights_file)
    acgn.discriminator.load_weights(discriminator_weights_file)
    images = acgn.train(train_dataset_small,100,10,20,1,25)
else:
    print('no wieght found')
    images = acgn.train(train_dataset_small,100,10,20,1,25)


# Current relevent runtime information:
{'__method__np_save': {'type': "<class 'numpy._ArrayFunctionDispatcher'>"},
 'images': {'dtype': 'float32',
            'execution_cell_source': {'cellno': 5, 'lineno': 33},
            'has_nan': False,
            'shape': (1, 20, 32, 32, 3),
            'type': 'numpy.ndarray',
            'value_range': (-0.09130533, 0.098355025)},
 'np': {'type': "<class 'module'>"}}
# Target Cell:




np.save('images.npy', images)