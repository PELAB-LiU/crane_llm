# Executed Cells:
## Cell 1:

import os
import shutil
import itertools
import pathlib
from PIL import Image


import cv2
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('whitegrid')
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix , classification_report


import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D , MaxPooling2D , Flatten , Activation , Dense , Dropout , BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam , Adamax
from tensorflow.keras import regularizers


import warnings
warnings.filterwarnings('ignore')

## Cell 2:


train_dir = 'data_small/Training'
test_dir = 'data_small/Testing'

## Cell 3:


train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

## Cell 4:

test_datagen = ImageDataGenerator(rescale=1./255)


train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

## Cell 5:

from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.models import Model

base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))


for layer in base_model.layers:
    layer.trainable = False


x = base_model.output
x = GlobalAveragePooling2D()(x)


inception_model = Model(inputs=base_model.input, outputs=x)

## Cell 6:

train_features = inception_model.predict(train_generator)
test_features = inception_model.predict(test_generator)

## Cell 7:

from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Reshape
from keras.layers import Bidirectional, LSTM
from tensorflow.keras.models import Model

## Cell 8:

from tensorflow.keras.utils import to_categorical


train_labels_one_hot = to_categorical(train_generator.classes, num_classes=4)
test_labels_one_hot = to_categorical(test_generator.classes, num_classes=4)

## Cell 9:

from tensorflow.keras.callbacks import EarlyStopping


early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

## Cell 10:

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Reshape, Conv2D, MaxPooling2D, Bidirectional, LSTM, Dropout, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from kerastuner.tuners import RandomSearch
from kerastuner.engine.hyperparameters import HyperParameters
from tensorflow.keras.optimizers import Adam, RMSprop, SGD

## Cell 11:


input_features = Input(shape=(2048,), name='input_features')


reshaped_features = Reshape((32, 64, 1))(input_features)


cnn_output = Conv2D(32, (3, 3), activation='relu')(reshaped_features)
cnn_output = MaxPooling2D(pool_size=(2, 2))(cnn_output)
cnn_output = Conv2D(64, (3, 3), activation='relu')(cnn_output)
cnn_output = MaxPooling2D(pool_size=(2, 2))(cnn_output)
cnn_output = Conv2D(128, (3, 3), activation='relu')(cnn_output)
cnn_output = MaxPooling2D(pool_size=(2, 2))(cnn_output)
cnn_output = Flatten()(cnn_output)

cnn_output_reshaped = Reshape((1, -1))(cnn_output)

bi_lstm_output = Bidirectional(LSTM(128, return_sequences=True))(cnn_output_reshaped)
bi_lstm_output = Bidirectional(LSTM(64, return_sequences=True))(bi_lstm_output)


bi_lstm_output_flatten = Flatten()(bi_lstm_output)

## Cell 12:


def build_model(hp):
    dense_units = hp.Int('dense_units', min_value=64, max_value=256, step=32)
    lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)
    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)
    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])
    batch_size = hp.Choice('batch_size', values=[16, 32, 64])

    if optimizer_choice == 'adam':
        optimizer = Adam(learning_rate=1e-3)
    elif optimizer_choice == 'rmsprop':
        optimizer = RMSprop(learning_rate=1e-3)
    else:
        optimizer = SGD(learning_rate=1e-3)
    dense_layer = Dense(dense_units, activation='relu')(bi_lstm_output_flatten)
    dense_layer = Dropout(dropout_rate)(dense_layer)
    dense_layer = Dense(64, activation='relu')(dense_layer)
    output = Dense(4, activation='softmax')(dense_layer)

    cnn_model = Model(inputs=input_features, outputs=output)
    cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return cnn_model

## Cell 13:


tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=3,
    directory='hyperparameter_tuning',
    project_name='cnn_model_tuning'
)


tuner.search(
    train_features,
    train_labels_one_hot,
    epochs=2,
    validation_data=(test_features, test_labels_one_hot),
    callbacks=[early_stopping]
)

# Current relevent runtime information:
{'__method__plt_figure': {'type': "<class 'function'>"},
 '__method__plt_legend': {'type': "<class 'function'>"},
 '__method__plt_scatter': {'type': "<class 'function'>"},
 '__method__plt_show': {'type': "<class 'function'>"},
 '__method__plt_title': {'type': "<class 'function'>"},
 '__method__plt_xlabel': {'type': "<class 'function'>"},
 '__method__plt_ylabel': {'type': "<class 'function'>"},
 '__method__tuner_get_best_models': {'type': "<class 'method'>"},
 '__method__tuner_oracle': {'type': '<class '
                                    "'keras_tuner.src.tuners.randomsearch.RandomSearchOracle'>"},
 'plt': {'type': "<class 'module'>"},
 'tuner': {'execution_cell_source': {'cellno': 17, 'lineno': 2},
           'type': '<class '
                   "'keras_tuner.src.tuners.randomsearch.RandomSearch'>"}}
# Target Cell:


best_model = tuner.get_best_models(1)[0]
best_model.summary()


best_trials = tuner.oracle.get_best_trials(5)


plt.figure(figsize=(10, 6))
for trial in best_trials:
    val_accuracy_history = trial.metrics.get_history(name='val_accuracy')



    plt.scatter(trial.trial_id, val_accuracy_history[0].value)

plt.title('Validation Accuracy of Best Trials')
plt.xlabel("Trial ID")
plt.ylabel('Validation Accuracy')
plt.legend()
plt.show()