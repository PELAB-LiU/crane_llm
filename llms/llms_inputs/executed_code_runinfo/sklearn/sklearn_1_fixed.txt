# Executed Cells:
## Cell 1:

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import skimage.io
import keras.backend as K
import tensorflow as tf
from tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv2D

## Cell 2:

train_datagen = ImageDataGenerator(
                                         width_shift_range = 0.1,
                                         height_shift_range = 0.1,
                                         horizontal_flip = True,
                                         rescale = 1./255,

                                         validation_split = 0.2
                                        )
valid_datagen = ImageDataGenerator(rescale = 1./255,
                                         validation_split = 0.2)
test_datagen = ImageDataGenerator(rescale = 1./255,
                                         validation_split = 0.2)

## Cell 3:

train_dataset=train_datagen.flow_from_directory(directory='data_small/train',
                                               target_size=(48,48),
                                               class_mode='categorical',
                                               subset='training',
                                               batch_size=64)

## Cell 4:

valid_dataset=valid_datagen.flow_from_directory(directory='data_small/test',
                                               target_size=(48,48),
                                               class_mode='categorical',
                                               batch_size=64)

## Cell 5:

test_dataset=test_datagen.flow_from_directory(directory='data_small/test',
                                               target_size=(48,48),
                                               class_mode='categorical',
                                               batch_size=64)

## Cell 6:

train_datagen = ImageDataGenerator(

    rotation_range=20,
    zoom_range=0.2
)

## Cell 7:

from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D
from tensorflow.keras.layers import SeparableConv2D
from tensorflow.keras.layers import concatenate
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras import Model
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.regularizers import l2

inputs=Input((64,64,3))

h=Conv2D(64,(1,1),padding='same',activation='relu')(inputs)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=Conv2D(64,(3,3))(h)
h=BatchNormalization()(h)

h=Activation('relu')(h)

b=Conv2D(128,(1,1),strides=(2,2))(h)
b=BatchNormalization()(b)

h=SeparableConv2D(128,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=Activation('relu')(h)
h=SeparableConv2D(128,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h=MaxPooling2D((2,2),strides=(2,2))(h)

h=concatenate([h,b],name='first')

b=Conv2D(128,(2,2),strides=(2,2))(h)
b=BatchNormalization()(b)

h=SeparableConv2D(128,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=SeparableConv2D(128,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h=MaxPooling2D((2,2),strides=(2,2))(h)

h=concatenate([h,b],name='second')

b=Conv2D(256,(1,1),padding='same')(h)
b=BatchNormalization()(b)
b=MaxPooling2D((2,2),strides=(2,2))(b)

h=SeparableConv2D(256,(3,3),padding='same')(h)
h=BatchNormalization()(h)

h=Activation('relu')(h)
h=SeparableConv2D(256,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h=MaxPooling2D((2,2),strides=(2,2))(h)

h=concatenate([h,b],name='third')
b=h

h = Activation('relu')(h)
h=SeparableConv2D(512,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=SeparableConv2D(512,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=SeparableConv2D(512,(3,3),padding='same')(h)
h=BatchNormalization()(h)

h=concatenate([h,b],name='fourth')

b=Conv2D(512,(1,1),padding='same')(h)
b=BatchNormalization()(b)
b=MaxPooling2D((2,2),strides=(2,2))(b)

h=SeparableConv2D(512,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=SeparableConv2D(512,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h=MaxPooling2D((2,2),strides=(2,2))(h)

h=concatenate([h,b],name='fifth')

b=Conv2D(1024,(1,1),padding='same')(h)
b=BatchNormalization()(b)
b=MaxPooling2D((2,2),strides=(2,2))(b)

h=SeparableConv2D(1024,(3,3),padding='same')(h)
h = Activation('relu')(h)
h=SeparableConv2D(1024,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h=MaxPooling2D((2,2),strides=(2,2))(h)

h=concatenate([h,b],name='sixth')
b=h

h = Activation('relu')(h)

h=SeparableConv2D(512,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=SeparableConv2D(256,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=SeparableConv2D(128,(3,3),padding='same')(h)
h=BatchNormalization()(h)

h=concatenate([h,b],name='seventh')
b=h

b=Conv2D(256,(1,1),strides=(1,1))(h)
b=BatchNormalization()(b)

h = Activation('relu')(h)
h=SeparableConv2D(1024,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)
h=SeparableConv2D(512,(3,3),padding='same')(h)
h=BatchNormalization()(h)

h=concatenate([h,b],name='eighth')

h=SeparableConv2D(256,(3,3),padding='same')(h)
h=BatchNormalization()(h)
h = Activation('relu')(h)



x = GlobalAveragePooling2D()(h)


x = Dense(1024)(x)
x = BatchNormalization()(x)
x = LeakyReLU(alpha=0.1)(x)
x = Dropout(0.4)(x)

x = Dense(512)(x)
x = BatchNormalization()(x)
x = LeakyReLU(alpha=0.1)(x)
x = Dropout(0.4)(x)


x = Dense(256)(x)
x = BatchNormalization()(x)
x = LeakyReLU(alpha=0.1)(x)
x = Dropout(0.3)(x)

x = Dense(128)(x)
x = BatchNormalization()(x)
x = LeakyReLU(alpha=0.1)(x)
x = Dropout(0.2)(x)



outputs = Dense(7, activation='softmax')(x)




model = Model(inputs=inputs, outputs=outputs)
model.summary()

## Cell 8:

def f1_score(y_true,y_pred):
    true_positives=K.sum(K.round(K.clip(y_true*y_pred,0,1)))
    possible_positives=K.sum(K.round(K.clip(y_true,0,1)))
    predicted_positives=K.sum(K.round(K.clip(y_pred,0,1)))
    precision=true_positives/(predicted_positives+K.epsilon())
    recall=true_positives/(possible_positives+K.epsilon())
    f1_val=2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

## Cell 9:

METRICS=[
    tf.keras.metrics.BinaryAccuracy(name='accuracy'),
    tf.keras.metrics.Precision(name='precision'),
    tf.keras.metrics.Recall(name='recall'),
    tf.keras.metrics.AUC(name='auc'),
      f1_score,
]

# Current relevent runtime information:
{'METRICS': {'execution_cell_source': {'cellno': 11, 'lineno': 1},
             'length': 5,
             'type': "<class 'list'>",
             'value_info': {'num_unique': 5,
                            'unique_values': [<BinaryAccuracy name=accuracy>,
                                              <Precision name=precision>,
                                              <Recall name=recall>,
                                              <AUC name=auc>,
                                              <function f1_score at 0x74a33f403880>],
                            'value_type': 'categorical or object'}},
 '__method__model_compile': {'type': "<class 'method'>"},
 '__method__np_unique': {'type': "<class 'numpy._ArrayFunctionDispatcher'>"},
 '__method__train_dataset_labels': {'dtype': 'int32',
                                    'has_nan': False,
                                    'shape': (560,),
                                    'type': 'numpy.ndarray',
                                    'value_info': {'num_unique': 7,
                                                   'value_type': 'categorical '
                                                                 'numeric(no '
                                                                 'more than 10 '
                                                                 'unique '
                                                                 'values)'},
                                    'value_range': (0, 6)},
 'model': {'execution_cell_source': {'cellno': 7, 'lineno': 160},
           'type': "<class 'keras.src.models.functional.Functional'>"},
 'np': {'type': "<class 'module'>"},
 'train_dataset': {'batch_size': 64,
                   'execution_cell_source': {'cellno': 2, 'lineno': 1},
                   'image_shape': (48, 48, 3),
                   'n_samples': 560,
                   'num_classes': 7,
                   'target_size': (48, 48),
                   'type': '<class '
                           "'keras.src.legacy.preprocessing.image.DirectoryIterator'>"}}
# Target Cell:

from sklearn.utils.class_weight import compute_class_weight



class_weights = compute_class_weight(class_weight="balanced",
                                     classes=np.unique(train_dataset.labels),
                                     y=train_dataset.labels)

class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}




model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=METRICS)