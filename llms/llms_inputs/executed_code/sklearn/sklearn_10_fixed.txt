# Executed Cells:
## Cell 1:





from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn.metrics import make_scorer

from sklearn import svm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

from sklearn import neighbors
from math import sqrt




import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


## Cell 2:


df = pd.read_csv('data/measures_v2.csv',
                 usecols=[0,1,2,3,4,5,6,7,8,9,10,11])
df.head(10)

## Cell 3:

X=df.drop("motor_speed", axis=1)
y=df["motor_speed"]

## Cell 4:

from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=42)

training=df.copy()

## Cell 5:

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.ensemble import RandomForestRegressor

from warnings import filterwarnings
filterwarnings('ignore')

## Cell 6:

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score,mean_squared_error

## Cell 7:

from xgboost import XGBRegressor,XGBModel
from sklearn.model_selection import train_test_split,cross_val_score,cross_validate,KFold

## Cell 8:

params = {}
params['tree_method'] = 'gpu_hist'
params['predictor'] = 'gpu_predictor'
params['n_jobs'] = 4

model = XGBRegressor()
model.fit(X_train,y_train)
y_pred = model.predict(X_test)

# Target Cell:

params = {}
params['tree_method'] = 'hist'
params['predictor'] = 'predictor'
params['n_jobs'] = 4



n_splits = 10
test_preds = None
kf_rmse = []
for fold, (train_idx, valid_idx) in enumerate(KFold(n_splits=n_splits, shuffle=True).split(X_train,y_train)):


    X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train.iloc[train_idx]
    X_valid_fold, y_valid_fold = X_train.iloc[valid_idx], y_train.iloc[valid_idx]


    model = XGBRegressor(**params)
    model.fit(X_train_fold, y_train_fold,
            eval_set=[(X_valid_fold, y_valid_fold)],
            eval_metric='rmse', verbose=False)


    valid_pred = model.predict(X_valid_fold)


    rmse = np.sqrt(mean_squared_error(y_valid_fold, valid_pred))

    print(f'Fold {fold+1}/{n_splits} RMSE: {rmse:.4f}')
    kf_rmse.append(rmse)


    if test_preds is None:
        test_preds = model.predict(X_test)
    else:

        test_preds += model.predict(X_test)

test_preds /= n_splits
print(f'Average KFold RMSE: {np.mean(np.array(kf_rmse)):.5f}')