# Executed Cells:
## Cell 1:





import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import math


import warnings
warnings.filterwarnings('ignore')




import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))




## Cell 2:

with open('data/raw/txt/romance/domCasmurro.txt', 'r', encoding='utf8') as f:
    data = f.read()

## Cell 3:

chars = list(set(data))

## Cell 4:

indexer = {char: index for (index, char) in enumerate(chars)}

## Cell 5:

indexed_data = []
for c in data:
    indexed_data.append(indexer[c])

print("Indexed extract: ", indexed_data[:50])
print("Length: ", len(indexed_data))

## Cell 6:

def index2onehot(batch):

    batch_flatten = batch.flatten()
    onehot_flat = np.zeros((batch.shape[0] * batch.shape[1], len(indexer)))
    onehot_flat[range(len(batch_flatten)), batch_flatten] = 1
    onehot = onehot_flat.reshape((batch.shape[0], batch.shape[1], -1))

    return onehot

## Cell 7:

import torch
from torch import nn
from torch import optim
import torch.nn.functional as F

## Cell 8:

class LSTM(nn.Module):
    def __init__(self, char_length, hidden_size, n_layers):
        super().__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.lstm = nn.LSTM(char_length, hidden_size, n_layers, batch_first=True)
        self.output = nn.Linear(hidden_size, char_length)

    def forward(self, x, states):
        out, states = self.lstm(x, states)
        out = out.contiguous().view(-1, self.hidden_size)
        out = self.output(out)

        return out, states

    def init_states(self, batch_size):
        hidden = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()
        cell = next(self.parameters()).data.new(self.n_layers, batch_size, self.hidden_size).zero_()
        states = (hidden, cell)

        return states

## Cell 9:

n_seq = 100
seq_length =  50
n_batches = math.floor(len(indexed_data) / n_seq / seq_length)

total_length = n_seq * seq_length * n_batches
x = indexed_data[:total_length]
x = np.array(x).reshape((n_seq,-1))

## Cell 10:

model = LSTM(len(chars), 256, 2)
model

## Cell 11:

loss_function = nn.CrossEntropyLoss()
torch.autograd.set_detect_anomaly(True)

optimizer = optim.Adam(model.parameters(), lr=0.001)
epochs = 20

# Target Cell:

losses = []

for e in range(1, epochs+1):
    states = model.init_states(n_seq)
    batch_loss = []

    for b in range(0, x.shape[1], seq_length):
        x_batch = x[:,b:b+seq_length]

        if b == x.shape[1] - seq_length:
            y_batch = x[:,b+1:b+seq_length]
            y_batch = np.hstack((y_batch, indexer["."] * np.ones((y_batch.shape[0],1))))
        else:
            y_batch = x[:,b+1:b+seq_length+1]

        x_onehot = torch.Tensor(index2onehot(x_batch))
        y = torch.Tensor(y_batch).view(n_seq * seq_length)

        pred, states = model(x_onehot, states)
        loss = loss_function(pred, y.long())
        optimizer.zero_grad()
        loss.backward(retain_graph=True)
        optimizer.step()

        batch_loss.append(loss.item())

    losses.append(np.mean(batch_loss))

    if e%1 == 0:
        print("epoch: ", e, "... Loss function: ", losses[-1])